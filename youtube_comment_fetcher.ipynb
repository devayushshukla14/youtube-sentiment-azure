{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArVUNQjKmU9D"
   },
   "source": [
    "# Youtube comment fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Y-PcC3jWZxLJ",
    "outputId": "3324c11f-6c2d-47ae-b59f-189ec8aaf831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-eventhub\n",
      "  Downloading azure_eventhub-5.15.0-py3-none-any.whl.metadata (73 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.169.0)\n",
      "Collecting azure-core>=1.27.0 (from azure-eventhub)\n",
      "  Downloading azure_core-1.34.0-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-eventhub) (4.13.2)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.27.0->azure-eventhub) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.27.0->azure-eventhub) (1.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2025.4.26)\n",
      "Downloading azure_eventhub-5.15.0-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading azure_core-1.34.0-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: azure-core, azure-eventhub\n",
      "Successfully installed azure-core-1.34.0 azure-eventhub-5.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-eventhub google-api-python-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_5gJ_axdKpB"
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from transformers import pipeline\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qnon5aYneRZ-"
   },
   "outputs": [],
   "source": [
    "YOUTUBE_API_KEY = '**********'\n",
    "VIDEO_ID = 'fsQgc9pCyDU'\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtQGl0adenRE"
   },
   "outputs": [],
   "source": [
    "EVENT_HUB_CONNECTION_STR = '*******'\n",
    "EVENT_HUB_NAME = 'comments-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npnrk_snfK6B"
   },
   "outputs": [],
   "source": [
    "producer = EventHubProducerClient.from_connection_string(\n",
    "    conn_str=EVENT_HUB_CONNECTION_STR,\n",
    "    eventhub_name=EVENT_HUB_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356,
     "referenced_widgets": [
      "a710b58f60b2476abc61a57ca7e30ab0",
      "52fa788405c041028fac393b84829250",
      "7389c9c036784584b35a6ca59abc5188",
      "8323dcc0f0be485dad4a745bc160c666",
      "a0a8b675519541309d7056b30edda52f",
      "2e08cd7c108646d6949d1f116ee2965c",
      "c62b03a3cbe94c27986276a6946fbc85",
      "ae2aaebc143945ca9528882482893995",
      "19efe32707df40b880e1d4f6b7fc20a7",
      "0820b9f36dea43939d70fd6d008456ff",
      "4eb3545b676842c0a1185895e2afb533",
      "568ad43394c1400a93fa5051146f619b",
      "35b8fc89d5f648f68325e75d2ed2f775",
      "e44268a2aaac4b86bfe415c2882c69e4",
      "13c3b94d63724c6a8bc7939aebecb574",
      "cdfdc01c2db54345afcef3d2b162e03e",
      "ee29260aafbb4e7b95b3fa263157bcf7",
      "897521206e104dfcaafb6727d67def14",
      "9640a26982ed49558674c42b66c7ad27",
      "afabdb0d373448fcbe23f49bbe4a062a",
      "bf1eeca9c24e423bb4659fd0b19d281a",
      "0210b57213394f35ab6d6b49168b3c8a",
      "d44cc4f474d14d19a41d801f0855f08a",
      "da0636ce9fca43a8a43596fecf362fc0",
      "611d9174354b44cbbc3492686311355f",
      "c44b1cb0eada46048939135b18292b6b",
      "00ce084b8766499e857da4ce9f216ea5",
      "acb13a969e5e431cbe758e038422fcf7",
      "271e52e63c3b428a965e915e6de6bd66",
      "40211afe5ea34186b9ca466758fef543",
      "880060bd38ed40c49d34c8c58ddde516",
      "add03beb3a994ed593138e8dd9bd1d85",
      "c56441162d254682a2282b3718f1ec59",
      "44f733504e2d4253b7cbe2416b205e03",
      "429c4b69fffc4aacb93c0bd46d96ff56",
      "17bd732e66a941b79c2d94eb8b817760",
      "23aea8d8059d4454bb4fedb3a3d85270",
      "74ad4b64b8d0458291bdf9114a7665db",
      "8d6dde11354641f1b47b2840499dfb4f",
      "e093782c7f3345409c30d1ca5131d045",
      "e709e113a36c4261aa081d0e20b05762",
      "fc534834ac5f46279b5a11add1df9b42",
      "59a8ef090a1e40f380ce18f7f2b9cfa9",
      "6a974c88920e4207859f9eaa783c76cd"
     ]
    },
    "id": "JCIaabgWsTqr",
    "outputId": "ea2fb936-e789-4e72-c00e-7346a9ef0701"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a710b58f60b2476abc61a57ca7e30ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568ad43394c1400a93fa5051146f619b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44cc4f474d14d19a41d801f0855f08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f733504e2d4253b7cbe2416b205e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcbKT0wHffVs"
   },
   "outputs": [],
   "source": [
    "def get_all_youtube_comments(video_id, max_total=1000):\n",
    "    \"\"\"\n",
    "    Fetch up to `max_total` comments with pagination,\n",
    "    returns list of dicts: [{\"text\": comment_text}, ...]\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_total:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token,\n",
    "            textFormat='plainText'\n",
    "        ).execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comments.append({\"text\": comment_text})\n",
    "            if len(comments) >= max_total:\n",
    "                break\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    print(f\"✅ Collected {len(comments)} comments\")\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JYG5-IfskJJ"
   },
   "outputs": [],
   "source": [
    "def huggingface_sentiment(text):\n",
    "    result = sentiment_model(text[:512])[0]  # HuggingFace model max token length safety\n",
    "    return result['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU5cPhNCsrxe"
   },
   "outputs": [],
   "source": [
    "def rule_based_sentiment(text):\n",
    "    text = text.lower()\n",
    "    positive_words = [\"good\", \"love\", \"great\", \"amazing\", \"awesome\"]\n",
    "    negative_words = [\"bad\", \"hate\", \"terrible\", \"worst\", \"awful\"]\n",
    "\n",
    "    if any(word in text for word in positive_words):\n",
    "        return \"Positive\"\n",
    "    elif any(word in text for word in negative_words):\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLH035DqrjUJ"
   },
   "outputs": [],
   "source": [
    "def send_to_event_hub(comments, batch_size=50):\n",
    "    from azure.eventhub import EventHubProducerClient, EventData\n",
    "    import json\n",
    "    import time\n",
    "\n",
    "    producer = EventHubProducerClient.from_connection_string(\n",
    "        conn_str=EVENT_HUB_CONNECTION_STR,\n",
    "        eventhub_name=EVENT_HUB_NAME\n",
    "    )\n",
    "\n",
    "    with producer:\n",
    "        for i in range(0, len(comments), batch_size):\n",
    "            batch_comments = comments[i:i + batch_size]\n",
    "            batch = producer.create_batch()\n",
    "\n",
    "            for comment in batch_comments:\n",
    "                text = comment[\"text\"]\n",
    "                comment[\"hf_sentiment\"] = huggingface_sentiment(text)\n",
    "                comment[\"sql_sentiment\"] = rule_based_sentiment(text)\n",
    "\n",
    "                event_data = EventData(json.dumps(comment))\n",
    "\n",
    "                try:\n",
    "                    batch.add(event_data)\n",
    "                except ValueError:\n",
    "                    # If a single event is too large, skip or log\n",
    "                    print(f\"⚠️ Skipping large comment at index {i}\")\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                producer.send_batch(batch)\n",
    "                print(f\"✅ Sent batch {i // batch_size + 1} with {len(batch_comments)} comments\")\n",
    "                time.sleep(0.2)  # Optional: slight delay to avoid timeouts\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error sending batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "    print(f\"✅ Sent total {len(comments)} comments to Event Hub\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwelh9CNfpMU",
    "outputId": "a6a666d7-c2e3-4375-fbae-3f1d82927a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collected 5355 comments\n",
      "✅ Sent batch 1 with 50 comments\n",
      "✅ Sent batch 2 with 50 comments\n",
      "✅ Sent batch 3 with 50 comments\n",
      "✅ Sent batch 4 with 50 comments\n",
      "✅ Sent batch 5 with 50 comments\n",
      "✅ Sent batch 6 with 50 comments\n",
      "✅ Sent batch 7 with 50 comments\n",
      "✅ Sent batch 8 with 50 comments\n",
      "✅ Sent batch 9 with 50 comments\n",
      "✅ Sent batch 10 with 50 comments\n",
      "✅ Sent batch 11 with 50 comments\n",
      "✅ Sent batch 12 with 50 comments\n",
      "✅ Sent batch 13 with 50 comments\n",
      "✅ Sent batch 14 with 50 comments\n",
      "✅ Sent batch 15 with 50 comments\n",
      "✅ Sent batch 16 with 50 comments\n",
      "✅ Sent batch 17 with 50 comments\n",
      "✅ Sent batch 18 with 50 comments\n",
      "✅ Sent batch 19 with 50 comments\n",
      "✅ Sent batch 20 with 50 comments\n",
      "✅ Sent batch 21 with 50 comments\n",
      "✅ Sent batch 22 with 50 comments\n",
      "✅ Sent batch 23 with 50 comments\n",
      "✅ Sent batch 24 with 50 comments\n",
      "✅ Sent batch 25 with 50 comments\n",
      "✅ Sent batch 26 with 50 comments\n",
      "✅ Sent batch 27 with 50 comments\n",
      "✅ Sent batch 28 with 50 comments\n",
      "✅ Sent batch 29 with 50 comments\n",
      "✅ Sent batch 30 with 50 comments\n",
      "✅ Sent batch 31 with 50 comments\n",
      "✅ Sent batch 32 with 50 comments\n",
      "✅ Sent batch 33 with 50 comments\n",
      "✅ Sent batch 34 with 50 comments\n",
      "✅ Sent batch 35 with 50 comments\n",
      "✅ Sent batch 36 with 50 comments\n",
      "✅ Sent batch 37 with 50 comments\n",
      "✅ Sent batch 38 with 50 comments\n",
      "✅ Sent batch 39 with 50 comments\n",
      "✅ Sent batch 40 with 50 comments\n",
      "✅ Sent batch 41 with 50 comments\n",
      "✅ Sent batch 42 with 50 comments\n",
      "✅ Sent batch 43 with 50 comments\n",
      "✅ Sent batch 44 with 50 comments\n",
      "✅ Sent batch 45 with 50 comments\n",
      "✅ Sent batch 46 with 50 comments\n",
      "✅ Sent batch 47 with 50 comments\n",
      "✅ Sent batch 48 with 50 comments\n",
      "✅ Sent batch 49 with 50 comments\n",
      "✅ Sent batch 50 with 50 comments\n",
      "✅ Sent batch 51 with 50 comments\n",
      "✅ Sent batch 52 with 50 comments\n",
      "✅ Sent batch 53 with 50 comments\n",
      "✅ Sent batch 54 with 50 comments\n",
      "✅ Sent batch 55 with 50 comments\n",
      "✅ Sent batch 56 with 50 comments\n",
      "✅ Sent batch 57 with 50 comments\n",
      "✅ Sent batch 58 with 50 comments\n",
      "✅ Sent batch 59 with 50 comments\n",
      "✅ Sent batch 60 with 50 comments\n",
      "✅ Sent batch 61 with 50 comments\n",
      "✅ Sent batch 62 with 50 comments\n",
      "✅ Sent batch 63 with 50 comments\n",
      "✅ Sent batch 64 with 50 comments\n",
      "✅ Sent batch 65 with 50 comments\n",
      "✅ Sent batch 66 with 50 comments\n",
      "✅ Sent batch 67 with 50 comments\n",
      "✅ Sent batch 68 with 50 comments\n",
      "✅ Sent batch 69 with 50 comments\n",
      "✅ Sent batch 70 with 50 comments\n",
      "✅ Sent batch 71 with 50 comments\n",
      "✅ Sent batch 72 with 50 comments\n",
      "✅ Sent batch 73 with 50 comments\n",
      "✅ Sent batch 74 with 50 comments\n",
      "✅ Sent batch 75 with 50 comments\n",
      "✅ Sent batch 76 with 50 comments\n",
      "✅ Sent batch 77 with 50 comments\n",
      "✅ Sent batch 78 with 50 comments\n",
      "✅ Sent batch 79 with 50 comments\n",
      "✅ Sent batch 80 with 50 comments\n",
      "✅ Sent batch 81 with 50 comments\n",
      "✅ Sent batch 82 with 50 comments\n",
      "✅ Sent batch 83 with 50 comments\n",
      "✅ Sent batch 84 with 50 comments\n",
      "✅ Sent batch 85 with 50 comments\n",
      "✅ Sent batch 86 with 50 comments\n",
      "✅ Sent batch 87 with 50 comments\n",
      "✅ Sent batch 88 with 50 comments\n",
      "✅ Sent batch 89 with 50 comments\n",
      "✅ Sent batch 90 with 50 comments\n",
      "✅ Sent batch 91 with 50 comments\n",
      "✅ Sent batch 92 with 50 comments\n",
      "✅ Sent batch 93 with 50 comments\n",
      "✅ Sent batch 94 with 50 comments\n",
      "✅ Sent batch 95 with 50 comments\n",
      "✅ Sent batch 96 with 50 comments\n",
      "✅ Sent batch 97 with 50 comments\n",
      "✅ Sent batch 98 with 50 comments\n",
      "✅ Sent batch 99 with 50 comments\n",
      "✅ Sent batch 100 with 50 comments\n",
      "✅ Sent batch 101 with 50 comments\n",
      "✅ Sent batch 102 with 50 comments\n",
      "✅ Sent batch 103 with 50 comments\n",
      "✅ Sent batch 104 with 50 comments\n",
      "✅ Sent batch 105 with 50 comments\n",
      "✅ Sent batch 106 with 50 comments\n",
      "✅ Sent batch 107 with 50 comments\n",
      "✅ Sent batch 108 with 5 comments\n",
      "✅ Sent total 5355 comments to Event Hub\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    VIDEO_ID = \"fsQgc9pCyDU\"\n",
    "    MAX_COMMENTS = 13000\n",
    "\n",
    "    comments = get_all_youtube_comments(VIDEO_ID, max_total=MAX_COMMENTS)\n",
    "    send_to_event_hub(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "Zntp49cOJxjl",
    "outputId": "5bd471a0-3527-4a48-b1ab-db6eab86cada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage-blob\n",
      "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (1.34.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (43.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (4.13.2)\n",
      "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2025.4.26)\n",
      "Downloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: isodate, azure-storage-blob\n",
      "Successfully installed azure-storage-blob-12.25.1 isodate-0.7.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "2ea3c07ee78848c68a38877fa2d0a00c",
       "pip_warning": {
        "packages": [
         "azure"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynZ7S6bfl-Ry"
   },
   "source": [
    "# Retrieving CSV from Blob Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OTYuW13JQyg",
    "outputId": "c3dbb957-8f04-47e1-8b68-d9655fe0d374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 1 blobs\n",
      "✅ Parsed 5355 records\n",
      "📁 Saved CSV to /content/youtube_sentiment_data.csv\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Your storage account connection string\n",
    "conn_str = \"************\"\n",
    "\n",
    "# The container name where Stream Analytics outputs blobs\n",
    "container_name = \"sentimentoutput\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# List all blobs\n",
    "blob_list = list(container_client.list_blobs())\n",
    "print(f\"✅ Found {len(blob_list)} blobs\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for blob in blob_list:\n",
    "    blob_client = container_client.get_blob_client(blob)\n",
    "    blob_data = blob_client.download_blob().readall()\n",
    "    blob_text = blob_data.decode('utf-8')\n",
    "\n",
    "    # Parse each line (JSONL)\n",
    "    for line in blob_text.strip().split('\\n'):\n",
    "        try:\n",
    "            all_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"⚠️ Could not parse a line in {blob.name}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "print(f\"✅ Parsed {len(df)} records\")\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = \"youtube_sentiment_data.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"📁 Saved CSV to {os.path.abspath(output_csv)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ijtf8ejiJnFt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
